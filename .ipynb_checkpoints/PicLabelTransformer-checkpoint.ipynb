{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fc6d4a0-b194-4718-976c-ae78a7dad024",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch import optim\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a79abb87-a0ac-4868-92c9-1ff47baaad1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_data=torchvision.datasets.MNIST(\n",
    "    root='MNIST',\n",
    "    train=True,\n",
    "    transform=torchvision.transforms.Compose([\n",
    "                                   torchvision.transforms.ToTensor(),\n",
    "                                   torchvision.transforms.Normalize((0.1307,), (0.3081,))\n",
    "                               ]),\n",
    "    download=True\n",
    ")\n",
    "test_data=torchvision.datasets.MNIST(\n",
    "    root='MNIST',\n",
    "    train=False,\n",
    "    transform=torchvision.transforms.Compose([\n",
    "                                   torchvision.transforms.ToTensor(),\n",
    "                                   torchvision.transforms.Normalize((0.1307,), (0.3081,))\n",
    "                               ]),\n",
    "    download=True\n",
    ")\n",
    "train_load=DataLoader(dataset=train_data,batch_size=128,shuffle=True)\n",
    "test_load=DataLoader(dataset=test_data,batch_size=128,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "076920fc-77cc-423d-8262-ceafc0eb35f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1570937f-a6b8-41ba-a26c-9a13b38f62ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class positionalEncoding(nn.Module):\n",
    "    def __init__(self, patch_num, embedding_dim):\n",
    "        super(positionalEncoding, self).__init__()\n",
    "        self.P = torch.zeros((1, patch_num, embedding_dim))\n",
    "        X = torch.arange(patch_num, dtype=torch.float32).reshape(-1, 1)/ torch.pow(10000, torch.arange(0, embedding_dim, 2, dtype=torch.float32) / embedding_dim)\n",
    "        self.P[:, :, 0::2] = torch.sin(X)\n",
    "        self.P[:, :, 1::2] = torch.cos(X)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.P[:, :x.shape[1], :].to(device)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b338ed0-fb3f-4981-9093-ed8e0d377f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selfAttention\n",
    "def selfAttention(queries, keys, values):\n",
    "    d = queries.shape[-1]\n",
    "    scores = torch.bmm(queries, keys.transpose(1, 2))/ math.sqrt(d)\n",
    "    return torch.bmm(nn.Softmax(dim=-1)(scores), values)\n",
    "\n",
    "def transpose_qkv(X, h):\n",
    "# 输入的X：batch_size * n * em_dim => b * n * h * e/h =>(b*h) * n * e/h\n",
    "    X = X.reshape(X.shape[0], X.shape[1], h, -1)\n",
    "    X = X.permute(0, 2, 1, 3)\n",
    "    return X.reshape(-1, X.shape[2], X.shape[3])\n",
    "\n",
    "def multiConcat(X, h):\n",
    "# 输入的X：(b*h) * n * e/h => b * h * n * e/h=> b * n * h * e/h => b * n * e\n",
    "    X = X.reshape(-1, h, X.shape[1], X.shape[2])\n",
    "    X = X.permute(0, 2, 1, 3)\n",
    "    return X.reshape(X.shape[0], X.shape[1], -1)\n",
    "\n",
    "# multi-head\n",
    "class multiHeadAttention(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, h):\n",
    "        super(multiHeadAttention, self).__init__()\n",
    "        self.h = h\n",
    "        self.x2q = nn.Linear(input_dim, embedding_dim, False)\n",
    "        self.x2k = nn.Linear(input_dim, embedding_dim, False)\n",
    "        self.x2v = nn.Linear(input_dim, embedding_dim, False)\n",
    "        self.selfAttention = selfAttention\n",
    "        self.wo = nn.Linear(embedding_dim, embedding_dim, False)\n",
    "        self.forRes = nn.Linear(embedding_dim, input_dim, False)\n",
    "    def forward(self, x):\n",
    "        q = self.x2q(x).to(device)\n",
    "        k = self.x2k(x).to(device)\n",
    "        v = self.x2v(x).to(device)\n",
    "        \n",
    "        queries = transpose_qkv(q, self.h)\n",
    "        keys = transpose_qkv(k, self.h)        \n",
    "        values = transpose_qkv(v, self.h)\n",
    "        \n",
    "        output = self.selfAttention(queries, keys, values)\n",
    "        output_concat = multiConcat(output,self.h)\n",
    "        final_output = self.forRes(self.wo(output_concat)) + x\n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5488fdc2-5151-4a9f-941c-de0863ed5bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.mlp_block = torch.nn.Sequential()\n",
    "        self.mlp_block.add_module(\"linear1\", nn.Linear(input_size, 1024))\n",
    "        self.mlp_block.add_module(\"ReLU1\", nn.ReLU())\n",
    "        self.mlp_block.add_module(\"linear2\", nn.Linear(1024, 512))\n",
    "        self.mlp_block.add_module(\"ReLU2\", nn.ReLU())\n",
    "        self.mlp_block.add_module(\"linear3\", nn.Linear(512, output_size))\n",
    "    def forward(self, x):\n",
    "        # print(f\"mlp_input_size{x.shape}\")\n",
    "        return F.softmax(self.mlp_block(x),dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd301d85-d698-496a-88dd-2415842b63d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, patch_num, embedding_dim, h, output_size):\n",
    "        super(Net, self).__init__()\n",
    "        #position_encoding\n",
    "        self.positionalEncoding = positionalEncoding(patch_num, embedding_dim)\n",
    "        #multi-ihead-attention\n",
    "        self.multihead = multiHeadAttention(embedding_dim, embedding_dim, h)\n",
    "        #mlp\n",
    "        self.mlp = MLP(patch_num * embedding_dim, output_size)\n",
    "    def forward(self, x):\n",
    "        # print(f\"ori input.shape{x.shape}\")\n",
    "        x = self.positionalEncoding(x)\n",
    "        # print(f\"after poscode,x.shape{x.shape}\")\n",
    "        x = self.multihead(x)\n",
    "        # print(f\"after multihead,x.shape{x.shape}\")\n",
    "        # n * w=>flatten\n",
    "        x = x.flatten(-2)\n",
    "        # print(f\"after flatten,x.shape{x.shape}\")\n",
    "        x = self.mlp(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81b78792-6b39-42a7-acdd-e576e0b063d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (positionalEncoding): positionalEncoding()\n",
      "  (multihead): multiHeadAttention(\n",
      "    (x2q): Linear(in_features=512, out_features=512, bias=False)\n",
      "    (x2k): Linear(in_features=512, out_features=512, bias=False)\n",
      "    (x2v): Linear(in_features=512, out_features=512, bias=False)\n",
      "    (wo): Linear(in_features=512, out_features=512, bias=False)\n",
      "    (forRes): Linear(in_features=512, out_features=512, bias=False)\n",
      "  )\n",
      "  (mlp): MLP(\n",
      "    (mlp_block): Sequential(\n",
      "      (linear1): Linear(in_features=8192, out_features=1024, bias=True)\n",
      "      (ReLU1): ReLU()\n",
      "      (linear2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "      (ReLU2): ReLU()\n",
      "      (linear3): Linear(in_features=512, out_features=10, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = Net(16, 512, 8, 10).to(device)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28d6e7c8-ec86-4489-b0da-615e64a425d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbed(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        img_size = (img_size, img_size)\n",
    "        patch_size = (patch_size, patch_size)\n",
    "        num_patches = (img_size[1] // patch_size[1]) * (img_size[0] // patch_size[0])\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        assert H == self.img_size[0] and W == self.img_size[1], \\\n",
    "          f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n",
    "#         print(f\"x  {x.shape}\")\n",
    "        x = self.proj(x)\n",
    "#         print(f\"proj  {x.shape}\")\n",
    "        x = x.flatten(2)\n",
    "#         print(f\"flatten    {x.shape}\")\n",
    "        x = x.transpose(1, 2)\n",
    "#         print(f\"trans{x.shape}\")\n",
    "        return x  \n",
    "    \n",
    "# x = torch.randn((100,1,28,28))\n",
    "# mypatchtest = PatchEmbed(28, 7, 1, 512)\n",
    "# res = mypatchtest(x)\n",
    "# print(res.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb73d6db-d1c4-46e7-8c70-04361527bb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epoches = 5\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.1)# Adam梯度下降\n",
    "lossCal = nn.CrossEntropyLoss()\n",
    "pic2Patches = PatchEmbed(28, 7, 1, 512).to(device)#res.size = torch.Size([batch_size, 16, 512])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed89e279-3d52-4350-b37b-1ec57042ae7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training network:  20%|█████████████▍                                                     | 1/5 [00:10<00:41, 10.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch number: (1, 5)loss：1.7041966554198438\n",
      "   corrrect78.3%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training network:  40%|██████████████████████████▊                                        | 2/5 [00:20<00:29,  9.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch number: (2, 5)loss：1.5538828538170755\n",
      "   corrrect91.44%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training network:  60%|████████████████████████████████████████▏                          | 3/5 [00:29<00:19,  9.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch number: (3, 5)loss：1.5346541203923825\n",
      "   corrrect93.23%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training network:  80%|█████████████████████████████████████████████████████▌             | 4/5 [00:38<00:09,  9.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch number: (4, 5)loss：1.5218887674783084\n",
      "   corrrect94.46%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training network: 100%|███████████████████████████████████████████████████████████████████| 5/5 [00:48<00:00,  9.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch number: (5, 5)loss：1.5130044295589553\n",
      "   corrrect95.26%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "acces = []\n",
    "eval_losses = []\n",
    "eval_acces = []\n",
    "\n",
    "for epoch in tqdm(range(num_epoches), desc = \"training network\"):\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    num_correct = 0\n",
    "    for img, label in train_load:\n",
    "        img = img.to(device)\n",
    "        # print(f\"berore topatch{img.shape}\")\n",
    "        img = pic2Patches(img)\n",
    "        # print(f\"after topatch{img.shape}\")\n",
    "        label = label.to(device)\n",
    "        # 前向传播\n",
    "        \n",
    "        res = net(img)\n",
    "        loss = lossCal(res, label)\n",
    "        # 反向传播\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # loss.item()是将零维张量转换为浮点数\n",
    "        train_loss += loss.item()\n",
    "        # 计算分类的准确率\n",
    "        _, pred = res.max(1)\n",
    "        num_correct += torch.sum(pred == label)\n",
    "        acc = num_correct / img.shape[0]\n",
    "        train_acc +=acc\n",
    "    losses.append(train_loss / len(train_load))\n",
    "    acces.append(train_acc / len(train_load))\n",
    "    tqdm.write(f\"epoch number: {epoch+1,num_epoches}loss：{train_loss/len(train_load)}\")\n",
    "    tqdm.write(f\"   corrrect{int(10000 * num_correct/len(train_data)) / 100}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6963435-ce80-4ea0-b6f5-dad094ee6431",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca54430-339b-4db9-be04-496bb965c3af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
